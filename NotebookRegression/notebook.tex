
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Round 4 - Model Validation and Selection}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Machine Learning: Basic
Principles}\label{machine-learning-basic-principles}

\subsection{Model Validation and
Selection}\label{model-validation-and-selection}

\subsection{Learning goals}\label{learning-goals}

Remember the main componets of a machine learning problem:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{data points} which are characterized by features
  \(\mathbf{x}\) and labels \(y\),
\item
  a \textbf{hyopothesis space} \(\mathcal{H}\) collecting feasible maps
  (predictors) \(h(\mathbf{x})\) from features \(\mathbf{x}\) to a
  predicted label \(\hat{y}=h(\mathbf{x})\) and
\item
  a \textbf{loss function} (such as squared error loss or the logistic
  loss).
\end{enumerate}

In this exercise you will learn a principled approach for \textbf{model
selection}, i.e., how to optimally choose the best (in a certain sense)
hypothesis space \(\mathcal{H}\) from a whole ensemble of different
hpyothesis spaces \(\mathcal{H}^{(1)},\mathcal{H}^{(2)},\ldots\). This
approach is based on computing, for each hypothesis space
\(\mathcal{H}^{(d)}\), the empirical risk (the validation error) of the
predictor \(\hat{h}\) obtained by \textbf{empirical risk minization}
over \(\mathcal{H}^{(d)}\). The data points used for computing the
validation error is known as \textbf{validation set} or
\textbf{cross-validation set} \(\mathcal{X}^{(\rm val)}\).

The validation set should contain data points which have not already
been used for choosing the predictor out of \(\mathcal{H}^{(d)}\) via
empirical risk minimization. We can then compare the quality of
different models by comparing the validation error incurred by the
predictors that have been obtained using the different models.

We will also learn about \textbf{regularization} as a "soft" variant of
model selection. Here, we use a fixed but large hypothesis space (e.g.,
space of polynomials with a large degree) and regularize the empirical
risk minimization problem by adding a term that quanties the
"complexity" of a particular predictor (e.g., a polynomial of high
degree).

The concept of model selection and regularization is best understood by
working through a particular example. We will use as this example the
problem of predicing cryptocurrencies which you already faced in Round 2
- "Regression".

\subsubsection{Exercise Content}\label{exercise-content}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Predicting Cryptocurrencies
\item
  Model Selection
\item
  Regularization
\end{enumerate}

\subsubsection{Keywords}\label{keywords}

\texttt{Regularization}, \texttt{Validation}, \texttt{Model\ Selection},
`Polynomial Regression', 'Regularized Empirical Risk Minimization'

    \subsection{1. Predicting A
Cryptocurrency}\label{predicting-a-cryptocurrency}

We consider again (see Round 2 "Regression") the problem of predicting
the closing price of the cryptocurrency Ethereum based on the closing
price of Bitcoin on the same day. However, we will only consider a
\textbf{subset} of the historic data about Ethereum and Bitcoin used in
round \texttt{2.Regression}. This subset of \(N=20\) data points
\(\mathcal{X} = \{ (x^{(i)},y^{(i)}) \}_{i=1}^{N}\) is stored in the
file \emph{BTC\_ETH\_round4.csv}.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k}{as} \PY{n+nn}{pd}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{display}
        
        \PY{c+c1}{\PYZsh{}Read the data}
        \PY{n}{df}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{BTC\PYZus{}ETH\PYZus{}round4.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{x}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{Bitcoin}\PY{o}{.}\PY{n}{values} \PY{c+c1}{\PYZsh{}Bitcoin values}
        \PY{n}{y}\PY{o}{=}\PY{n}{df}\PY{o}{.}\PY{n}{Ethereum}\PY{o}{.}\PY{n}{values} \PY{c+c1}{\PYZsh{}Ethereum values}
        
        \PY{c+c1}{\PYZsh{}Reshape the data. }
        \PY{c+c1}{\PYZsh{}This is really important for the matrix multiplication later on!}
        \PY{n}{x}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
        \PY{n}{y}\PY{o}{=}\PY{n}{np}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{y}\PY{p}{,}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}Plot the data}
        \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)} 
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{bf}\PY{l+s+s1}{\PYZob{}}\PY{l+s+s1}{Figure}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{ 1.\PYZcb{}\PYZdl{} Bitcoin vs Ethereum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Bitcoin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ethereum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
<Figure size 600x600 with 1 Axes>
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]
20

    \end{Verbatim}

    \subsection{2 Model Selection}\label{model-selection}

As observed in Round 2 ("Regression"), the relation between the Ethereum
price \(y\) and the Bitcoin price \(x\) is highly non-linear. Therefore
it is useful to consider a hypothesis space which is constituted by
polynomial functions of degree \(d\):

\begin{equation}
\label{equ_def_poly_hyposapce}
\mathcal{H}^{(d)}_{\rm poly} = \{ h^{(\mathbf{w})}(\cdot): \mathbb{R} \rightarrow \mathbb{R}: h^{(\mathbf{w})}(x) = \sum_{r=1}^{d+1} w_{r} x^{r-1} \mbox{, with some } \mathbf{w} =(w_{1},\ldots,w_{d+1})^{T} \in \mathbb{R}^{d+1} \}.
\end{equation}

As discussed in the course book (Section 3.2 "Polynomial Regression"),
polynomial regression is equivalent to combining linear regression with
a feature map. In particular, we transform the features \(x\) (Bitcoin
closing price) of the data points to a higher dimensional feature space
using the feature map

\begin{equation}
\phi(x) = (x^{0},\ldots,x^{d-1})^{T} \in \mathbb{R}^{d}.
\label{eq7}
\tag{7}
\end{equation}

Using this new feature vector \(\mathbf{x}=\phi(x)\), we can represent
any polynomial map \(h \in \mathcal{H}^{(d)}_{\rm poly}\) as

\begin{equation} 
h^{(\mathbf{w})}(x) = \mathbf{w}^{T} \phi(x) \mbox{ with some weight vector } \mathbf{w} \in \mathbb{R}^{d+1}.
\label{eq8}
\tag{8}
\end{equation}

Consider a particular choice for the maximum degree (e.g., \(d=2\)). We
can find a good polynomial predictor \(\hat{h}^{(d)}\) by minimizing the
empirical risk over some labeled datapoints (the training set). However,
we will not use the entire dataset \(\mathcal{X}\) for this empirical
risk minimization but only a subset, the training set

\begin{equation} 
\mathcal{X}^{(\rm train)} = \{ (x^{(2)},y^{(2)}), (x^{(4)},y^{(4)}), (x^{(6)},y^{(6)}),\ldots,(x^{(N)},y^{(N)}) \}.
\end{equation}

This training set \(\mathcal{X}^{(\rm train)}\), of size
\(N_{\rm train} = N/2\), is obtained by selecting every other data point
of the original data set \(\mathcal{X}\). The remaining data points in
\(\mathcal{X}\) will be used as the validation set:

\begin{equation} 
\mathcal{X}^{(\rm val)}= \{(x^{(1)},y^{(1)}),(x^{(3)},y^{(3)}),(x^{(19)},y^{(19)}).
\end{equation}

For each choide of \(d\), which corresponds to a particular hypothesis
space \(\mathcal{H}^{(d)}\), we learn the optimal predcitor
\(\hat{h}^{(d)}\) by solving

\begin{align}
\hat{h}^{(d)} & = {\rm arg} \min_{h \in \mathcal{H}^{(d)}_{\rm poly}} (1/N_{\rm train}) \sum_{i=2,4,\ldots,20} (y^{(i)} - h(x^{(i)})^2. 
\label{eq9}
\tag{9}
\end{align}

This optimization problem can be rewritten using the paramtrization
\eqref{eq7} as

\begin{align}
\hat{\mathbf{w}}^{(d)} & = {\rm arg} \min_{\mathbf{w} \in \mathbb{R}^{d+1}} (1/N_{\rm train}) \sum_{i=2,4,\ldots,20} \big(y^{(i)} - \mathbf{w}^{T}\phi \big(x^{(i)}\big) \big)^2. 
\label{eq10}
\tag{10}
\end{align}

Given the optimal weight vector \(\hat{\mathbf{w}}^{(d)}\) which solves
\eqref{eq10}, we obtain the optimal predictor \(\hat{h}^{(d)}\) (which
solves \eqref{eq9}) by
\(\hat{h}^{(d)}(x) = \big( \hat{\mathbf{w}}^{(d)} \big)^{T} \phi(x)\).

This optimization problem can solved either using a closed-form
expression (under certain conditions) involving the feature matrix
\(\mathbf{X}=\big(\phi\big(x^{(2)}\big),\phi\big(x^{(4)}\big),\ldots,\phi\big(x^{(20)}\big)\big)^{T}\)
or using gradient descent (with a sufficiently small step size). Let us
denote the training error, which is the minimum objective value in
\eqref{eq9} and \eqref{eq10}, obtained for degree \(d\) as
\(E_{\rm train}^{(d)}\).

After finding the predictor \(\hat{h}^{(d)}\) for different choices of
the maximum degree \(d\), we evaluate the empirical risk incurred by
\(\hat{h}^{(d)}\) over the validation set:

\begin{equation}
E_{\rm val}^{(d)} = (1/N_{\rm val}) \sum_{i=1,3,\ldots,19} \big(y^{(i)} - \mathbf{w}^{T}\phi\big(x^{(i)}\big) \big)^2.
\end{equation}

We then choose the degree \(d\) for which the validation error
\(E_{\rm val}^{(d)}\) is smallest. Note that we do not use the training
error \(E_{\rm train}^{(d)}\) for choosing the degree \(d\). In general
it is not a good idea to choose the hypothesis space based on how small
the training error is. This is because for very large hypothesis spaces,
like polynomials with a large degree, we can always find a predictor
from it which \textbf{by accident} is able to fit the training data
well. However, such a predictor will perform poorly when applied to
other data points, such as the data points in the validation set.

\begin{itemize}
\tightlist
\item
  The Python function \texttt{trainValErrors()} below takes as input the
  training data, validation data and a list of degrees. And gives as
  output 3 vectors which contain for every degree the optimal weight,
  \emph{w\_opts}, the training error, \emph{training\_errors} and the
  validation error, \emph{validation\_errors} :

  \begin{itemize}
  \tightlist
  \item
    Use the given functions, these are the same as the ones used for
    Polynomial Regression in round 2.
  \item
    First get the optimal weights by only using the training data
  \item
    Then compute the empirical risk separately for the training data and
    validation data
  \end{itemize}
\end{itemize}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{}Calculate the optimal weight vector}
            \PY{n}{w\PYZus{}opt} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{y}\PY{p}{)}
            \PY{k}{return} \PY{n}{w\PYZus{}opt}
        
        \PY{k}{def} \PY{n+nf}{feature\PYZus{}mapping}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{degree}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{}compute the feature map for the given degree}
            \PY{n}{polynomial\PYZus{}features} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{column\PYZus{}stack}\PY{p}{(}\PY{p}{[}\PY{n}{x}\PY{o}{*}\PY{o}{*}\PY{n}{d} \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{degree}\PY{p}{)}\PY{p}{]}\PY{p}{)}
            \PY{k}{return} \PY{n}{polynomial\PYZus{}features}
        
        \PY{k}{def} \PY{n+nf}{polynomialRegression}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{degree}\PY{p}{)}\PY{p}{:}
            \PY{n}{X} \PY{o}{=} \PY{n}{feature\PYZus{}mapping}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{degree}\PY{p}{)}
            \PY{n}{w\PYZus{}opt}\PY{o}{=}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{)}
            \PY{k}{return} \PY{n}{w\PYZus{}opt}
        
        \PY{k}{def} \PY{n+nf}{predict}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{w\PYZus{}opt}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{}predict the labels}
            \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{w\PYZus{}opt}\PY{p}{)}
            \PY{k}{return} \PY{n}{y\PYZus{}pred}
        
        \PY{k}{def} \PY{n+nf}{empirical\PYZus{}risk}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w\PYZus{}opt}\PY{p}{)}\PY{p}{:}
            \PY{n}{empirical\PYZus{}error} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{subtract}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{w\PYZus{}opt}\PY{p}{)}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
            \PY{k}{return} \PY{n}{empirical\PYZus{}error}
                           
        
        \PY{c+c1}{\PYZsh{}Split the data into a training and validation set. }
        \PY{c+c1}{\PYZsh{}Don\PYZsq{}t change this.}
        \PY{n}{x\PYZus{}train}\PY{o}{=}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}
        \PY{n}{x\PYZus{}val}\PY{o}{=}\PY{n}{x}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}
        \PY{n}{y\PYZus{}train}\PY{o}{=}\PY{n}{y}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}
        \PY{n}{y\PYZus{}val}\PY{o}{=}\PY{n}{y}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}
        
        \PY{c+c1}{\PYZsh{}the degrees we want to loop over}
        \PY{n}{degrees}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,}\PY{l+m+mi}{4}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{]}
        
        \PY{k}{def} \PY{n+nf}{trainValErrors}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{x\PYZus{}val}\PY{p}{,}\PY{n}{y\PYZus{}val}\PY{p}{,}\PY{n}{degrees}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} STUDENT TASK \PYZsh{}\PYZsh{}\PYZsh{}}
            \PY{c+c1}{\PYZsh{}compute the optimal weight, training and validation error for each degree in degrees}
            \PY{n}{w\PYZus{}opts} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{training\PYZus{}errors}\PY{o}{=}\PY{p}{[}\PY{p}{]}
            \PY{n}{validation\PYZus{}errors}\PY{o}{=}\PY{p}{[}\PY{p}{]}
            \PY{k}{for} \PY{n}{d} \PY{o+ow}{in} \PY{n}{degrees}\PY{p}{:}
                \PY{n}{w\PYZus{}opt}\PY{o}{=}\PY{n}{polynomialRegression}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{d}\PY{p}{)}
                \PY{n}{w\PYZus{}opts}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{w\PYZus{}opt}\PY{p}{)}
                \PY{n}{training\PYZus{}errors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{empirical\PYZus{}risk}\PY{p}{(}\PY{n}{feature\PYZus{}mapping}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,}\PY{n}{d}\PY{p}{)}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{w\PYZus{}opt}\PY{p}{)}\PY{p}{)}
                \PY{n}{validation\PYZus{}errors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{empirical\PYZus{}risk}\PY{p}{(}\PY{n}{feature\PYZus{}mapping}\PY{p}{(}\PY{n}{x\PYZus{}val}\PY{p}{,}\PY{n}{d}\PY{p}{)}\PY{p}{,}\PY{n}{y\PYZus{}val}\PY{p}{,}\PY{n}{w\PYZus{}opt}\PY{p}{)}\PY{p}{)}
            \PY{k}{return} \PY{n}{w\PYZus{}opts}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,}\PY{n}{validation\PYZus{}errors}
        
        \PY{c+c1}{\PYZsh{}compute the training and validation errors and display them}
        \PY{n}{w\PYZus{}opts}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,} \PY{n}{validation\PYZus{}errors} \PY{o}{=} \PY{n}{trainValErrors}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{x\PYZus{}val}\PY{p}{,}\PY{n}{y\PYZus{}val}\PY{p}{,}\PY{n}{degrees}\PY{p}{)}
        \PY{n}{df\PYZus{}degrees}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{d}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{degrees}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{E\PYZus{}train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{training\PYZus{}errors}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{E\PYZus{}val}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{validation\PYZus{}errors}\PY{p}{\PYZcb{}}\PY{p}{)}
        \PY{n}{display}\PY{p}{(}\PY{n}{df\PYZus{}degrees}\PY{p}{)}
        
        \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
        \PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{121}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}plot the predictions for the different degrees}
        \PY{n}{x\PYZus{}plot} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x\PYZus{}val}\PY{p}{,}\PY{n}{y\PYZus{}val}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{degrees}\PY{p}{)}\PY{p}{)}\PY{p}{:}
            \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}plot}\PY{p}{,} \PY{n}{predict}\PY{p}{(}\PY{n}{feature\PYZus{}mapping}\PY{p}{(}\PY{n}{x\PYZus{}plot}\PY{p}{,}\PY{n}{degrees}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{n}{w\PYZus{}opts}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{degree = }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}}\PY{k}{degrees}[i]) 
            
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{bf}\PY{l+s+s1}{\PYZob{}}\PY{l+s+s1}{Figure}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{ 2.\PYZcb{}\PYZdl{} Polynomial Regression for different degrees}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Bitcoin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ethereum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}plot the training and validation errors for the different degrees}
        \PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{122}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{degrees}\PY{p}{,}\PY{n}{training\PYZus{}errors}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{degrees}\PY{p}{,}\PY{n}{validation\PYZus{}errors}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{bf}\PY{l+s+s1}{\PYZob{}}\PY{l+s+s1}{Figure}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{ 3.\PYZcb{}\PYZdl{} Training and validation error for different degrees}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Empirical error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Degree}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{degrees}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
   d   E_train     E_val
0  1  0.047725  0.085636
1  2  0.005044  0.018037
2  3  0.003882  0.013717
3  4  0.003613  0.021369
4  5  0.003581  0.013433
5  6  0.002970  1.346183
    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_4_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{3 Regularization}\label{regularization}

In Section 2 we have tried to find the best hypothesis space within a
sequence of increasing spaces
\(\mathcal{H}^{(1)} \subseteq \mathcal{H}^{(2)} \subseteq \ldots \subseteq \mathcal{H}^{(d_{\rm max})}\).
Indeed, the space of polynomials with maximum degree \(d=1\) is a subset
of the space of all polynomials with maximum degree \(d=2\). The simple
but powerful idea behind model selection is to use the validation errors
obtained for the different hypothesis spaces and picking the one
resulting in the smallest validation error. An alternative approach to
model selection is to use the largest hypothesis space
\(\mathcal{H}^{(d_{\rm max})}\) and to \textbf{regularize} the
corresponding empirical risk minimization problem:

\begin{equation}
\hat{\mathbf{w}}^{(\lambda)} = {\rm arg} \min\limits_{h \in \mathcal{H}^{(d_{\rm max})}} (1/N) \sum_{i=1}^{N} (y^{(i)} - \mathbf{w}^{T} \phi(x^{(i)} )^{2}+\lambda \|\mathbf{w}\|^{2}_{2}. 
\label{eq11}
\tag{11}
\end{equation}

The function \(\mathcal{R}(h)\) measures the \textbf{complexity} of a
particular predictor map \(h\). There are many different choices for
this complexity function \(\mathcal{R}(h)\) which mainly depend on the
particular application at hand. Here, we will consider a particular
choice which has proven useful for polynomial regression. To this end,
note that in polynomial regression we can parametrize a predictor as
\(h^{(\mathbf{w})}(x)= \mathbf{w}^{T} \phi(x) = \sum_{l=0}^{d} w_{l+1} x^{l}\)
with some weight vector \(\mathbf{w} \in \mathbb{R}^{d+1}\). The entries
\(w_{l}\) of the weight vector \(\mathbf{w}=(w_{1},\ldots,w_{d+1})^{T}\)
are the cefficients of the powers \(x^{0},x^{1},\ldots,x^{d}\) of the
feature \(x\) (e.g., the Bitcoin closing price).

It is reasonable to measure the complexity of \(h^{(\mathbf{w})}\) by
the average size of the coeffcients \(w_{l}\). Indeed, for many large
coefficients \(w_{l}\), the polynomial map \(h^{(\mathbf{w})}(x)\) tends
to be more ''wobbly''. In particular, we will use the regularizer
\(\mathcal{R}(h^{(\mathbf{w})}) = \| \mathbf{w} \|^{2}_{2} = w_{1}^{2}+\ldots+w_{d+1}^{2}\),
which results in the following \textbf{regularized empirical risk
minimization problem}

\begin{equation}
\hat{\mathbf{w}}^{(\lambda)} = {\rm arg} \min\limits_{\mathbf{w} \in \mathbb{R}^{d+1}} (1/N_{\rm train}) \sum_{i=2,4,\ldots,20} (y^{(i)} - \mathbf{w}^{T} \phi(x^{(i)}))^{2}+\lambda \| \mathbf{w} \|^{2}_{2}.
\label{eq12}
\tag{12}
\end{equation}

The resulting weight vector \(\hat{\mathbf{w}}^{(\lambda)}\) is then
used to construct a predictor \(\hat{h}^{(\lambda)}\) as

\begin{equation} 
\hat{h}^{(\lambda)}(x) = \big( \hat{\mathbf{w}}^{(\lambda)}\big)^{T} \phi(x).
\label{eq13}
\tag{13}
\end{equation}

The constant \(\lambda \geq 0\) is a tuning parameter and controls the
effective degree of the rsulting predictor \(\hat{h}^{(\lambda)}(x)\).
If we choose a large value of \(\lambda\), we will obtain a predictor
\(\hat{h}^{(\lambda)}(x)\) which resembles a polynomial of small degree
(say, \(d=2\)). In contrast, if we choose a very small value of
\(\lambda\), we will typically obtain a predictor
\(\hat{h}^{(\lambda)}\) which resembles a high-degree polynomial.

In a certain sense, we replace (or approximate) the computation of the
validation error incurred by \(h^{(\mathbf{w})}\) by adding the
regularization term \(\lambda \| \mathbf{w} \|^{2}_{2}\) to the training
error. The term \(\lambda \| \mathbf{w} \|^{2}_{2}\) in \eqref{eq12}
accounts for the expected increase in validation error due to the
complexity of the predictor
\(h^{(\mathbf{w})}(x)= \mathbf{w}^{T} \phi(x)\).

Similar to linear regression, we can derive a closed-form solution for
the optimal weights in \eqref{eq12} as

\begin{equation}
\nonumber
\mathbf{w}_{\rm opt} = \text{argmin}_{\mathbf{w} \in  \mathbb{R}^{d_{\rm max}+1}} \big[(1/N_{\rm train}) \|\mathbf{y} - \mathbf{X} \mathbf{w} \|^{2}_{2} + \lambda \| \mathbf{w} \|^{2}_{2}\big]
\end{equation}\begin{equation} 
\Rightarrow\mathbf{w}_{\rm opt} = (1/N_{\rm train})( (1/N_{\rm train}) \mathbf{X}^{T} \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^{T} \mathbf{y}.
\label{eq14}
\tag{14}
\end{equation}

Here, we used the feature matrix
\(\mathbf{X} = \big(\phi(x^{(2)}),\phi(x^{(4)}),\ldots,\phi(x^{(20)}) \big)^{T} \in \mathbb{R}^{10 \times (d_{\rm max}+1)}\).

It remains to specify the choice of the regularization parameter
\(\lambda\) in \eqref{eq12}. One option is to try out different values
for \(\lambda\), compute the weight vector
\(\hat{\mathbf{w}}^{(\lambda)}\) by solving the problem \eqref{eq12} and
evaluate the validation error of the corresponding predictor
\(h^{(\lambda)}\) (see \eqref{eq13}):

\begin{equation}
E_{\rm val}^{(\lambda)} = (1/N_{\rm val}) \sum_{i=1,3,\ldots,19} (y^{(i)} - \mathbf{w}^{T} \phi(x^{(i)}))^{2}.
\end{equation}

We then pick the value for \(\lambda\) which results in the smallest
validation error \(E_{\rm val}^{(\lambda)}\).

\subsubsection{Tasks}\label{tasks}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement a Python function \texttt{regularizedFit()} which takes as
  input \(\mathbf{X}\), \(\mathbf{y}\) and \(\lambda\) and returns the
  optimal weight vector according to (Eq. \ref{eq14}).

  \begin{itemize}
  \tightlist
  \item
    You can look at the numpy functions being used in \texttt{fit()}
    implemented in Section 2.
  \item
    You can use \texttt{np.eye()} to generate an identity matrix
  \end{itemize}
\item
  Implement a Python function \texttt{regularizedPolynomialRegression()}
  which takes as input \(\mathbf{x}\),\(\mathbf{y}\),\(\lambda\) and the
  degree. It should return the optimal weight vector.

  \begin{itemize}
  \tightlist
  \item
    Use \texttt{feature\_mapping()} from Section 2 to get the feature
    matrix.
  \item
    Use \texttt{regularizedFit()} to compute the optimal weight vector.
  \end{itemize}
\item
  Implement a Python function \texttt{trainValErrorsRegularization()}
  which takes as inputs the training data, validation data, the list of
  lambdas and the degree. And gives as output 3 vectors which contain
  for every lambda the optimal weight, \emph{w\_opts}, the training
  error, \emph{training\_errors} and the validation error,
  \emph{validation\_errors}.

  \begin{itemize}
  \tightlist
  \item
    You can largely reuse \texttt{trainValErrors()} from Section 2. The
    difference is that you should now loop over the different lambda
    values, instead of the different degrees.
  \end{itemize}
\end{enumerate}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k}{def} \PY{n+nf}{regularizedFit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,}\PY{n}{l}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} STUDENT TASK \PYZsh{}\PYZsh{}\PYZsh{}}
            \PY{n}{N}\PY{o}{=}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
            \PY{n}{w\PYZus{}opt} \PY{o}{=} \PY{l+m+mi}{1}\PY{o}{/}\PY{n}{N}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{inv}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{/}\PY{n}{N}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{o}{+}\PY{n}{l}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{eye}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{y}\PY{p}{)}
            \PY{k}{return} \PY{n}{w\PYZus{}opt}
        
        \PY{k}{def} \PY{n+nf}{regularizedPolynomialRegression}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{l}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{degree}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} STUDENT TASK \PYZsh{}\PYZsh{}\PYZsh{}}
            \PY{n}{X} \PY{o}{=} \PY{n}{feature\PYZus{}mapping}\PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{degree}\PY{p}{)}
            \PY{n}{w\PYZus{}opt}\PY{o}{=}\PY{n}{regularizedFit}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{l}\PY{p}{)}
            \PY{k}{return} \PY{n}{w\PYZus{}opt}
        
        \PY{c+c1}{\PYZsh{}specify the degree}
        \PY{n}{degree}\PY{o}{=}\PY{l+m+mi}{6}
        \PY{c+c1}{\PYZsh{}specify list of values for lambda to be considered}
        \PY{n}{lambdas} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mf}{0.01}\PY{p}{,}\PY{l+m+mf}{0.5}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]} 
            
        \PY{k}{def} \PY{n+nf}{trainValErrorsRegularization}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{x\PYZus{}val}\PY{p}{,}\PY{n}{y\PYZus{}val}\PY{p}{,}\PY{n}{lambdas}\PY{o}{=}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{degree}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
            \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{} STUDENT TASK \PYZsh{}\PYZsh{}\PYZsh{}}
            \PY{c+c1}{\PYZsh{}compute the optimal weight, training and validation error for each lambda}
            \PY{n}{w\PYZus{}opts} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{training\PYZus{}errors}\PY{o}{=}\PY{p}{[}\PY{p}{]}
            \PY{n}{validation\PYZus{}errors}\PY{o}{=}\PY{p}{[}\PY{p}{]}
            \PY{k}{for} \PY{n}{l} \PY{o+ow}{in} \PY{n}{lambdas}\PY{p}{:}
                \PY{n}{w\PYZus{}opt}\PY{o}{=}\PY{n}{regularizedPolynomialRegression}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{l}\PY{p}{,}\PY{n}{degree}\PY{p}{)}
                \PY{n}{w\PYZus{}opts}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{w\PYZus{}opt}\PY{p}{)}
                \PY{n}{training\PYZus{}errors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{empirical\PYZus{}risk}\PY{p}{(}\PY{n}{feature\PYZus{}mapping}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,}\PY{n}{degree}\PY{p}{)}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{w\PYZus{}opt}\PY{p}{)}\PY{p}{)}
                \PY{n}{validation\PYZus{}errors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{empirical\PYZus{}risk}\PY{p}{(}\PY{n}{feature\PYZus{}mapping}\PY{p}{(}\PY{n}{x\PYZus{}val}\PY{p}{,}\PY{n}{degree}\PY{p}{)}\PY{p}{,}\PY{n}{y\PYZus{}val}\PY{p}{,}\PY{n}{w\PYZus{}opt}\PY{p}{)}\PY{p}{)}
            \PY{k}{return} \PY{n}{w\PYZus{}opts}\PY{p}{,} \PY{n}{training\PYZus{}errors}\PY{p}{,}\PY{n}{validation\PYZus{}errors}
        
        \PY{c+c1}{\PYZsh{}compute the training and validation errors and display them}
        \PY{n}{w\PYZus{}opts\PYZus{}reg}\PY{p}{,} \PY{n}{training\PYZus{}errors\PYZus{}reg}\PY{p}{,} \PY{n}{validation\PYZus{}errors\PYZus{}reg} \PY{o}{=} \PY{n}{trainValErrorsRegularization}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,}\PY{n}{y\PYZus{}train}\PY{p}{,}\PY{n}{x\PYZus{}val}\PY{p}{,}\PY{n}{y\PYZus{}val}\PY{p}{,}\PY{n}{lambdas}\PY{p}{,}\PY{n}{degree}\PY{o}{=}\PY{n}{degree}\PY{p}{)}
        \PY{n}{df\PYZus{}lambdas}\PY{o}{=}\PY{n}{pd}\PY{o}{.}\PY{n}{DataFrame}\PY{p}{(}\PY{n}{data}\PY{o}{=}\PY{p}{\PYZob{}}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Lambdas}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{lambdas}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training errors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{training\PYZus{}errors\PYZus{}reg}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation errors}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}\PY{n}{validation\PYZus{}errors\PYZus{}reg}\PY{p}{\PYZcb{}}\PY{p}{)}
        \PY{n}{display}\PY{p}{(}\PY{n}{df\PYZus{}lambdas}\PY{p}{)}
        
        \PY{n}{fig} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{16}\PY{p}{,} \PY{l+m+mi}{8}\PY{p}{)}\PY{p}{)}
        \PY{n}{ax} \PY{o}{=} \PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{121}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x\PYZus{}train}\PY{p}{,} \PY{n}{y\PYZus{}train}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x\PYZus{}val}\PY{p}{,}\PY{n}{y\PYZus{}val}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation data}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{x\PYZus{}plot} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{lambdas}\PY{p}{)}\PY{p}{)}\PY{p}{:}
            \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}plot}\PY{p}{,} \PY{n}{predict}\PY{p}{(}\PY{n}{feature\PYZus{}mapping}\PY{p}{(}\PY{n}{x\PYZus{}plot}\PY{p}{,}\PY{n}{degree}\PY{p}{)}\PY{p}{,}\PY{n}{w\PYZus{}opts\PYZus{}reg}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{} }\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{lambda=}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{\PYZdl{}}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}}\PY{k}{str}(lambdas[i])) 
            
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{bf}\PY{l+s+s1}{\PYZob{}}\PY{l+s+s1}{Figure}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{ 4.\PYZcb{}\PYZdl{} Regularized Polynomial Regression}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Bitcoin}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ethereum}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{}plot the training and validation errors for the different values of lambda}
        \PY{n}{ax}\PY{o}{=}\PY{n}{fig}\PY{o}{.}\PY{n}{add\PYZus{}subplot}\PY{p}{(}\PY{l+m+mi}{122}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{lambdas}\PY{p}{,}\PY{n}{training\PYZus{}errors\PYZus{}reg}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Training error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{lambdas}\PY{p}{,}\PY{n}{validation\PYZus{}errors\PYZus{}reg}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{label}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Validation error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+sa}{r}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{bf}\PY{l+s+s1}{\PYZob{}}\PY{l+s+s1}{Figure}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{ 5.\PYZcb{}\PYZdl{} Training and validation error for different lambdas}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Lambda}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Empirical error}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{xticks}\PY{p}{(}\PY{n}{lambdas}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
   Lambdas  Training errors  Validation errors
0     0.00         0.002970           1.346183
1     0.01         0.006321           0.021683
2     0.50         0.033803           0.056465
3     1.00         0.048999           0.076654
4     2.00         0.068277           0.101182
5     5.00         0.092993           0.131519
    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_6_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
